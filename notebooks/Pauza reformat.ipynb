{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454ef516-7cb6-4de4-9f61-3303dd67a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xml, json, spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95031bc-74a0-43b4-ae2a-b895b602e417",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'hr_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhr_core_news_sm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diachronic/lib/python3.12/site-packages/spacy/__init__.py:51\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     28\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     29\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     35\u001b[39m ) -> Language:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diachronic/lib/python3.12/site-packages/spacy/util.py:472\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'hr_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('hr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1022a-c266-4f14-aa00-ccaa395bd9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pauza_root = '../data/cropinion/pauza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a8fcb3-3bc4-4e18-9819-6d243b9867c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Lemmatizer():\n",
    "    def __init__(self):\n",
    "        self.lemmas = {}\n",
    "        # __init__ should ensure the file is downloaded on import\n",
    "        self.pattern = re.compile(\"\\w+\")\n",
    "        with open(\"../data/molex/molex14_word2lemma.txt\") as fin:\n",
    "            for line in fin:\n",
    "                word, lemma = line.split()\n",
    "                self.lemmas[word] = lemma\n",
    "\n",
    "    def lemmatize_word(self, word):\n",
    "        lemma = self.lemmas.get(word.lower())\n",
    "        if lemma:\n",
    "            return lemma\n",
    "        else:\n",
    "            return word[:5]\n",
    "\n",
    "    def lemmatize_string(self, string):\n",
    "        lemmas = []\n",
    "        for token in re.findall(self.pattern, string.lower()):\n",
    "            lemmas.append(self.lemmatize_word(token))\n",
    "        return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3719ba37-d958-4d82-9eff-4f74f6a458f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word(tagged_word):\n",
    "    word = tagged_word.find('Word').text\n",
    "    tag = tagged_word.find('POSTag').text\n",
    "    lemma = tagged_word.find('Lemma').text\n",
    "    stem = tagged_word.find('BasicStem').text\n",
    "\n",
    "    return word, tag, lemma, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3610d0a-eb34-4f80-9753-3a7ac2267751",
   "metadata": {},
   "outputs": [],
   "source": [
    "molex = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ab6922-b93f-46d1-aeef-5dbd0429cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 3310/3310 [01:03<00:00, 52.18it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tags = set()\n",
    "instance_files = os.listdir(pauza_root)\n",
    "raw_documents = {}\n",
    "\n",
    "pos_tag_words = {}\n",
    "\n",
    "for instance_file in tqdm(instance_files, total=len(instance_files)):\n",
    "    if not instance_file.endswith('xml'): continue\n",
    "    # print(instance_file)\n",
    "    docid = instance_file.replace(\".xml\", \"\")\n",
    "    fp = f\"{pauza_root}/{instance_file}\"\n",
    "    tree = ET.parse(fp)\n",
    "    root = tree.getroot()\n",
    "    # print(root.tag, root.attrib)\n",
    "    text = root.find('Text').text\n",
    "    rating = root.find('Rating').text\n",
    "\n",
    "    doc = nlp(text)\n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        word = token.text\n",
    "        pos = token.pos_.lower()\n",
    "        lemma = molex.lemmatize_word(word.lower()) \n",
    "        # lemmatize\n",
    "        doc_tokens.append((lemma, pos))\n",
    "        \n",
    "    raw_documents[docid] = {\n",
    "        'text':doc_tokens,\n",
    "        'rating':rating,\n",
    "        'docid': docid,\n",
    "    }\n",
    "\n",
    "    #for child in root.findall('TaggedWords'):\n",
    "    #    for word in child:\n",
    "    #        w, t, l, s = parse_word(word)\n",
    "    #        \n",
    "    #        if t not in pos_tag_words:\n",
    "    #            pos_tag_words[t] = {}\n",
    "    #        if w not in pos_tag_words[t]:\n",
    "    #            pos_tag_words[t][w] = 0\n",
    "    #        pos_tag_words[t][w] += 1\n",
    "        #print(\" \".join(document))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f6e6058-92e2-4be9-a923-1d924d3bfe41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A [('sve', 716), (':', 687), ('...', 449), ('super', 426), ('odlična', 343), ('ukusna', 199), ('fino', 193), (';', 188), ('..', 179), ('odlično', 178), ('ukusno', 153), ('brza', 150), ('dobra', 138), ('jako', 127), ('+', 127), ('toplo', 125), ('fina', 122), ('odličan', 121), ('svemu', 120), ('ljubazan', 118)]\n",
      "N [('dostava', 906), ('pizza', 689), ('hrana', 590), ('vrijeme', 464), ('dostavljač', 312), ('minuta', 283), ('vremena', 271), ('put', 263), ('mina', 249), ('pohvale', 243), ('palačinke', 242), ('sat', 240), ('quot', 230), (':', 211), ('porcija', 181), ('puta', 176), ('pizzu', 171), ('porcije', 169), ('čast', 162), ('pizze', 154)]\n",
      "Z [(',', 6161), ('.', 4719), (')', 1418), ('!', 1396), ('-', 596), ('(', 585), ('*', 276), ('&', 247), (\"'\", 75), ('/', 74), ('%', 12)]\n",
      "S [('u', 1276), ('na', 1087), ('za', 955), ('od', 749), ('s', 628), ('sa', 394), ('iz', 200), ('bez', 180), ('po', 180), ('nakon', 138), ('uz', 123), ('do', 118), ('umjesto', 88), ('zbog', 84), ('kod', 81), ('o', 70), ('prije', 66), ('preko', 45), ('oko', 28), ('ispod', 26)]\n",
      "R [('samo', 363), ('još', 334), ('jako', 333), ('uvijek', 280), ('malo', 265), ('više', 247), ('prije', 225), ('tako', 217), ('već', 190), ('danas', 172), ('stvarno', 146), ('vrlo', 145), ('inače', 138), ('opet', 130), ('definitivno', 104), ('sada', 102), ('baš', 97), ('nikad', 97), ('sad', 96), ('dobro', 92)]\n",
      "C [('i', 3811), ('da', 1304), ('ali', 756), ('a', 742), ('kao', 310), ('jer', 229), ('nego', 195), ('ni', 190), ('pa', 189), ('ili', 137), ('ako', 129), ('kako', 117), ('no', 112), ('iako', 75), ('te', 71), ('niti', 62), ('ma', 55), ('kad', 44), ('dakle', 43), ('osim', 41)]\n",
      "V [('je', 3071), ('sam', 972), ('su', 921), ('nije', 439), ('bilo', 382), ('...', 364), ('bila', 334), ('smo', 329), (':', 280), ('naručio', 275), ('bi', 265), ('došla', 201), ('stigla', 200), ('bio', 186), ('naručili', 185), ('nisam', 182), ('naručujem', 178), ('dostave', 161), ('naručila', 152), ('..', 150)]\n",
      "Q [('ne', 680), ('evo', 47), ('li', 44), ('eto', 32), ('god', 22), ('eno', 1)]\n",
      "P [('se', 940), ('to', 437), ('što', 387), ('mi', 353), ('svaka', 199), ('koji', 177), ('ovo', 137), ('nešto', 130), ('ja', 127), ('koje', 123), ('svaki', 108), ('koja', 102), ('vas', 100), ('me', 99), ('meni', 97), ('vam', 96), ('im', 93), ('ih', 87), ('ovaj', 81), ('ga', 76)]\n",
      "M [('<num>', 1659), ('sto', 128), ('dva', 64), ('dvije', 62), ('jedna', 62), ('jedan', 51), ('jednu', 49), ('tri', 29), ('jednom', 22), ('jedne', 22), ('jedno', 18), ('6/6', 17), ('oba', 14), ('pet', 13), ('jednog', 9), ('jedni', 8), ('desetak', 8), ('15-ak', 7), ('5/6', 7), ('osam', 7)]\n",
      "Y [('br', 7), ('cm', 6), ('j', 3), ('o', 2), ('g', 2), ('PDV-a', 1), ('god', 1)]\n",
      "I [('bome', 6), ('gle', 2), ('zbogom', 2), ('jao', 1), ('ah', 1)]\n"
     ]
    }
   ],
   "source": [
    "for tag, tag_freqs in pos_tag_words.items():\n",
    "    print(tag, sorted(tag_freqs.items(), key=lambda t:-t[1])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e3851-f6e7-4e5e-91d5-abe3d9bb68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS = {'I': 'INTJ', \n",
    "            'P': 'DET', \n",
    "            'Y', \n",
    "            'N' : 'NOUN', \n",
    "            'M': 'NUM', \n",
    "            'V': 'VERB', \n",
    "            'S': 'ADP', \n",
    "            'Z' : 'PUNCT',\n",
    "            'R', 'ADV',\n",
    "            'Q': 'PART', \n",
    "            'C': 'CCONJ', \n",
    "            'A': 'ADJ',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb32900-f6ad-4f2c-ae8f-dfece244b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_name = '../data/cropinion/pauza_clean_tokens.jsonl'\n",
    "\n",
    "with open(out_name, 'w') as outfile:\n",
    "    for k, instance in raw_documents.items():\n",
    "        outfile.write(json.dumps(instance)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d65b262-cc67-4cb3-95b3-3cac9aca2c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [['savršen', 'propn'], ['hrana', 'noun'], [',', 'punct'], ['dostava', 'verb'], ['točan', 'adj'], ['u', 'adp'], ['minuta', 'noun'], ['.', 'punct'], ['vrao', 'adv'], ['bogat', 'adj'], ['porcija', 'noun'], ['.', 'punct'], ['pristojan', 'noun'], ['i', 'cconj'], ['na', 'adp'], ['vriti', 'noun'], ['.', 'punct'], ['misliti', 'verb'], ['dati', 'sconj'], ['ne', 'part'], ['postojati', 'verb'], ['bolj', 'adv'], ['od', 'adp'], ['toga', 'det'], ['.', 'punct'], ['uz', 'adp'], ['sve', 'det'], ['to', 'det'], ['dolaziti', 'verb'], ['od', 'adp'], ['dostavljač', 'noun'], ['u', 'adp'], ['zalogajnica', 'noun'], ['.', 'punct'], ['tako', 'adv'], ['dati', 'sconj'], ['morati', 'verb'], ['pohvaliti', 'noun'], ['makar', 'adv'], ['ste', 'aux'], ['&', 'punct'], ['quot;', 'punct'], [';', 'punct'], ['!', 'punct']], 'rating': '6', 'docid': 'comment889'}\n"
     ]
    }
   ],
   "source": [
    "with open(out_name, 'r') as infile:\n",
    "    for line in infile:\n",
    "        print(json.loads(line))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce57859-fa43-4d16-ab57-c00dcd92fdce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diachronic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
